{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install yfinance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from yfinance import download\n",
    "from datetime import datetime\n",
    "from pandas.plotting import lag_plot\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.api import OLS, add_constant\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Define the date range for fetching data\n",
    "start_date = '2024-01-01'\n",
    "end_date = '2024-12-31'\n",
    "\n",
    "# Fetch data using yfinance\n",
    "def fetch_stock_data(ticker):\n",
    "    return download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Get FAANG data\n",
    "fb = fetch_stock_data('META')\n",
    "aapl = fetch_stock_data('AAPL')\n",
    "amzn = fetch_stock_data('AMZN')\n",
    "nflx = fetch_stock_data('NFLX')\n",
    "goog = fetch_stock_data('GOOG')\n",
    "\n",
    "# Get S&P 500 data\n",
    "sp = fetch_stock_data('^GSPC')\n",
    "\n",
    "# Get bitcoin data in USD\n",
    "bitcoin = fetch_stock_data('BTC-USD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group FAANG stocks\n",
    "def group_stocks(stock_dict):\n",
    "    return pd.concat({key: stock[\"Close\"] for key, stock in stock_dict.items()}, axis=1)\n",
    "\n",
    "# Data groups\n",
    "faang = group_stocks(\n",
    "    {\n",
    "        'Facebook': fb, \n",
    "        'Apple': aapl, \n",
    "        'Amazon': amzn, \n",
    "        'Netflix': nflx, \n",
    "        'Google': goog\n",
    "    }\n",
    ")\n",
    "\n",
    "faang_sp = group_stocks(\n",
    "    {\n",
    "        'Facebook': fb, \n",
    "        'Apple': aapl, \n",
    "        'Amazon': amzn, \n",
    "        'Netflix': nflx, \n",
    "        'Google': goog,\n",
    "        'S&P 500': sp\n",
    "    }\n",
    ")\n",
    "\n",
    "all_assets = group_stocks(\n",
    "    {\n",
    "        'Bitcoin': bitcoin,\n",
    "        'S&P 500': sp,\n",
    "        'Facebook': fb, \n",
    "        'Apple': aapl, \n",
    "        'Amazon': amzn, \n",
    "        'Netflix': nflx, \n",
    "        'Google': goog\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\alanm\\\\OneDrive\\\\Documents\\\\MADS\\\\Capstone1\\\\GitHub_Project\\\\BTC-USD_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 23\u001b[0m\n\u001b[0;32m     12\u001b[0m file_paths \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmazon\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124malanm\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMADS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCapstone1\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGitHub_Project\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAMZN_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApple\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124malanm\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mMADS\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mCapstone1\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mGitHub_Project\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mAAPL_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS&P 500\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124malanm\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mHands-On-Data-Analysis-with-Pandas-2nd-edition\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mHands-On-Data-Analysis-with-Pandas-2nd-edition-1\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mch_07\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m^GSPC_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m }\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Clean all datasets\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m datasets \u001b[38;5;241m=\u001b[39m {key: clean_dataset(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m, key) \u001b[38;5;28;01mfor\u001b[39;00m key, path \u001b[38;5;129;01min\u001b[39;00m file_paths\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Define tickers to analyze\u001b[39;00m\n\u001b[0;32m     26\u001b[0m tickers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMETA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAAPL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAMZN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNFLX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGOOG\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\alanm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alanm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\alanm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alanm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\alanm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\alanm\\\\OneDrive\\\\Documents\\\\MADS\\\\Capstone1\\\\GitHub_Project\\\\BTC-USD_data'"
     ]
    }
   ],
   "source": [
    "# Function to clean and process datasets\n",
    "def clean_dataset(df, key):\n",
    "    \"\"\"Clean the dataset by removing unnecessary rows and renaming columns.\"\"\"\n",
    "    df = df.iloc[2:].reset_index(drop=True)\n",
    "    df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
    "    df['Ticker'] = key  # Add a column for the ticker symbol\n",
    "    df['Date'] = pd.to_datetime(df['Date'])  # Convert Date to datetime\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')  # Ensure numeric columns\n",
    "    return df\n",
    "\n",
    "# Define file paths for all datasets\n",
    "file_paths = {\n",
    "    \"Amazon\": r\"C:\\Users\\alanm\\OneDrive\\Documents\\MADS\\Capstone1\\GitHub_Project\\AMZN_data.csv\",\n",
    "    \"Apple\": \"C:\\\\Users\\\\alanm\\\\OneDrive\\\\Documents\\\\MADS\\\\Capstone1\\\\GitHub_Project\\\\AAPL_data.csv\",\n",
    "    \"Bitcoin\": \"C:\\\\Users\\\\alanm\\\\OneDrive\\\\Documents\\\\MADS\\\\Capstone1\\\\GitHub_Project\\\\BTC-USD_data\",\n",
    "    \"Facebook\": \"C:\\\\Users\\\\alanm\\\\OneDrive\\\\Documents\\\\Hands-On-Data-Analysis-with-Pandas-2nd-edition\\\\Hands-On-Data-Analysis-with-Pandas-2nd-edition-1\\\\ch_07\\\\META_data.csv\",\n",
    "    \"Google\": \"C:\\\\Users\\\\alanm\\\\OneDrive\\\\Documents\\\\Hands-On-Data-Analysis-with-Pandas-2nd-edition\\\\Hands-On-Data-Analysis-with-Pandas-2nd-edition-1\\\\ch_07\\\\GOOG_data.csv\",\n",
    "    \"Netflix\": \"C:\\\\Users\\\\alanm\\\\OneDrive\\\\Documents\\\\Hands-On-Data-Analysis-with-Pandas-2nd-edition\\\\Hands-On-Data-Analysis-with-Pandas-2nd-edition-1\\\\ch_07\\\\NFLX_data.csv\",\n",
    "    \"S&P 500\": \"C:\\\\Users\\\\alanm\\\\OneDrive\\\\Documents\\\\Hands-On-Data-Analysis-with-Pandas-2nd-edition\\\\Hands-On-Data-Analysis-with-Pandas-2nd-edition-1\\\\ch_07\\\\^GSPC_data.csv\"\n",
    "}\n",
    "\n",
    "# Clean all datasets\n",
    "datasets = {key: clean_dataset(pd.read_csv(path), key) for key, path in file_paths.items()}\n",
    "\n",
    "# Define tickers to analyze\n",
    "tickers = ['META', 'AAPL', 'AMZN', 'NFLX', 'GOOG']\n",
    "\n",
    "# Plot 1: Moving Average Plots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "for ax, ticker in zip(axes.flatten(), tickers):\n",
    "    data = datasets[ticker]\n",
    "    data['20D_MA'] = data['Close'].rolling(window=20).mean()  # Calculate 20-day moving average\n",
    "    ax.plot(data['Date'], data['Close'], label=f'{ticker} Close')\n",
    "    ax.plot(data['Date'], data['20D_MA'], linestyle='--', label='20D MA')\n",
    "    ax.set_title(f'{ticker} Close and 20D MA')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Price')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Histograms of Close Prices\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "for ax, ticker in zip(axes.flatten(), tickers):\n",
    "    data = datasets[ticker]\n",
    "    sns.histplot(data['Close'], kde=True, ax=ax, bins=20)\n",
    "    ax.set_title(f'{ticker} Close Price Distribution')\n",
    "    ax.set_xlabel('Price')\n",
    "    ax.set_ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Boxplot of Close Prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "boxplot_data = [datasets[ticker]['Close'].dropna() for ticker in tickers]\n",
    "plt.boxplot(boxplot_data, labels=tickers)\n",
    "plt.title('Boxplot of Close Prices')\n",
    "plt.xlabel('Tickers')\n",
    "plt.ylabel('Price')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the dataset directory\n",
    "dataset_dir = r\"C:\\\\Users\\\\alanm\\\\OneDrive\\\\Documents\\\\Hands-On-Data-Analysis-with-Pandas-2nd-edition\\\\Hands-On-Data-Analysis-with-Pandas-2nd-edition-1\\\\ch_07\"\n",
    "\n",
    "# Define file names for all datasets\n",
    "file_names = {\n",
    "    \"Amazon\": \"AMZN_data.csv\",\n",
    "    \"Apple\": \"AAPL_data.csv\",\n",
    "    \"Bitcoin\": \"BTC-USD_data.csv\",\n",
    "    \"Facebook\": \"META_data.csv\",\n",
    "    \"Google\": \"GOOG_data.csv\",\n",
    "    \"Netflix\": \"NFLX_data.csv\",\n",
    "    \"S&P 500\": \"^GSPC_data.csv\"\n",
    "}\n",
    "\n",
    "# Define a function to process each dataset and compute summary statistics\n",
    "def process_data(file_path):\n",
    "    # Read data\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Ensure the necessary columns exist and clean data\n",
    "    df = df.rename(columns=lambda x: x.strip())\n",
    "    if 'Close' in df.columns:\n",
    "        df['Close'] = pd.to_numeric(df['Close'], errors='coerce')  # Ensure 'Close' is numeric\n",
    "        return df['Close'].describe()  # Compute summary statistics for the 'Close' column\n",
    "\n",
    "# Read and process all datasets into a group_stocks structure\n",
    "all_assets = {\n",
    "    name: process_data(os.path.join(dataset_dir, file_name))\n",
    "    for name, file_name in file_names.items()\n",
    "}\n",
    "\n",
    "# Convert to DataFrame and structure rows and columns as specified\n",
    "summary_df = pd.DataFrame(all_assets).T\n",
    "summary_df.rename(\n",
    "    columns={\n",
    "        \"count\": \"Count\",\n",
    "        \"mean\": \"Mean\",\n",
    "        \"std\": \"Std\",\n",
    "        \"min\": \"Min\",\n",
    "        \"25%\": \"25%\",\n",
    "        \"50%\": \"50%\",\n",
    "        \"75%\": \"75%\",\n",
    "        \"max\": \"Max\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Ensure the rows are named properly\n",
    "summary_df.index.name = \"Stock\"\n",
    "\n",
    "# Display the final summary DataFrame\n",
    "from IPython.display import display\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data using yfinance\n",
    "def fetch_stock_data(ticker):\n",
    "    data = download(ticker, start=start_date, end=end_date)\n",
    "    data.columns = [f\"{ticker}_{col}\" for col in data.columns]  # Prefix columns with ticker symbol\n",
    "    return data\n",
    "\n",
    "# Fetch data again to ensure column names are updated\n",
    "fb = fetch_stock_data('META')\n",
    "aapl = fetch_stock_data('AAPL')\n",
    "amzn = fetch_stock_data('AMZN')\n",
    "nflx = fetch_stock_data('NFLX')\n",
    "goog = fetch_stock_data('GOOG')\n",
    "sp = fetch_stock_data('^GSPC')\n",
    "bitcoin = fetch_stock_data('BTC-USD')\n",
    "\n",
    "# Debug: Check the updated column names\n",
    "print(\"Flattened Columns in AAPL DataFrame:\")\n",
    "print(aapl.columns)\n",
    "\n",
    "# Check for the closing price column\n",
    "closing_column = None\n",
    "for col in aapl.columns:\n",
    "    if 'Close' in col:\n",
    "        closing_column = col\n",
    "        break\n",
    "\n",
    "if closing_column is None:\n",
    "    raise KeyError(\"Column for closing prices not found. Available columns: \" + ', '.join(aapl.columns))\n",
    "\n",
    "# Example plot: Apple stock closing prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x=aapl.index, y=aapl[closing_column], color=\"blue\")\n",
    "plt.title(\"Apple Stock Closing Prices\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Closing Price\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define the dataset directory\n",
    "dataset_dir = r\"C:\\\\Users\\\\alanm\\\\OneDrive\\\\Documents\\\\Hands-On-Data-Analysis-with-Pandas-2nd-edition\\\\Hands-On-Data-Analysis-with-Pandas-2nd-edition-1\\\\ch_07\"\n",
    "\n",
    "# Define file names for all datasets\n",
    "file_names = {\n",
    "    \"Amazon\": \"AMZN_data.csv\",\n",
    "    \"Apple\": \"AAPL_data.csv\",\n",
    "    \"Bitcoin\": \"BTC-USD_data.csv\",\n",
    "    \"Facebook\": \"META_data.csv\",\n",
    "    \"Google\": \"GOOG_data.csv\",\n",
    "    \"Netflix\": \"NFLX_data.csv\",\n",
    "    \"S&P 500\": \"^GSPC_data.csv\"\n",
    "}\n",
    "\n",
    "# Load datasets and extract 'Close' prices\n",
    "data = {}\n",
    "for name, file_name in file_names.items():\n",
    "    file_path = os.path.join(dataset_dir, file_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.rename(columns=lambda x: x.strip())\n",
    "    if 'Close' in df.columns:\n",
    "        df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "        data[name] = df['Close']\n",
    "\n",
    "# Create a DataFrame with all assets' 'Close' prices\n",
    "close_prices = pd.DataFrame(data)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = close_prices.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap of Stock Prices\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean the datasets\n",
    "def clean_dataset(filepath, column_names):\n",
    "    \"\"\"\n",
    "    Cleans a dataset by:\n",
    "    - Removing the first two rows (assumed to be metadata).\n",
    "    - Renaming columns to a standardized format.\n",
    "    - Converting numeric columns to proper data types.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Remove metadata rows and reset index\n",
    "    df = df[2:].reset_index(drop=True)\n",
    "\n",
    "    # Assign proper column names\n",
    "    df.columns = column_names\n",
    "\n",
    "    # Convert 'Date' column to datetime\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # Convert numeric columns to appropriate types\n",
    "    numeric_columns = [col for col in column_names if col != 'Date']\n",
    "    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Define the directory for datasets and filepaths\n",
    "dataset_dir = r\"C:\\\\Users\\\\alanm\\\\OneDrive\\\\Documents\\\\Hands-On-Data-Analysis-with-Pandas-2nd-edition\\\\Hands-On-Data-Analysis-with-Pandas-2nd-edition-1\\\\ch_07\"\n",
    "\n",
    "# Filepaths and column names for each dataset\n",
    "datasets_info = {\n",
    "    \"^GSPC\": (f\"{dataset_dir}\\\\^GSPC_data.csv\", ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']),\n",
    "    \"AAPL\": (f\"{dataset_dir}\\\\AAPL_data.csv\", ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']),\n",
    "    \"AMZN\": (f\"{dataset_dir}\\\\AMZN_data.csv\", ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']),\n",
    "    \"BTC-USD\": (f\"{dataset_dir}\\\\BTC-USD_data.csv\", ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']),\n",
    "    \"GOOG\": (f\"{dataset_dir}\\\\GOOG_data.csv\", ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']),\n",
    "    \"META\": (f\"{dataset_dir}\\\\META_data.csv\", ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']),\n",
    "    \"NFLX\": (f\"{dataset_dir}\\\\NFLX_data.csv\", ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']),\n",
    "}\n",
    "\n",
    "# Clean all datasets\n",
    "cleaned_data = {name: clean_dataset(filepath, columns) for name, (filepath, columns) in datasets_info.items()}\n",
    "\n",
    "# Visualization Code\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pairplot for S&P 500 data\n",
    "sns.pairplot(cleaned_data['^GSPC'][['High', 'Low', 'Close']], diag_kind=\"kde\", kind=\"scatter\", corner=True)\n",
    "plt.suptitle(\"S&P 500 Pairplot\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Time series plot for Bitcoin data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cleaned_data['BTC-USD']['Date'], cleaned_data['BTC-USD']['Close'], label='Bitcoin Close Price')\n",
    "plt.title('Bitcoin Close Prices Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap for Amazon data\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cleaned_data['AMZN'][['Close', 'High', 'Low', 'Open', 'Volume']].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Amazon Data Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Line plot for FAANG stock prices (Close)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for name in ['AAPL', 'AMZN', 'GOOG', 'META', 'NFLX']:\n",
    "    plt.plot(cleaned_data[name]['Date'], cleaned_data[name]['Close'], label=f'{name} Close Price')\n",
    "plt.title('FAANG Close Prices Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data inspection (example for Apple stock)\n",
    "print(aapl.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(aapl.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data using yfinance\n",
    "def fetch_stock_data(ticker):\n",
    "    data = download(ticker, start=start_date, end=end_date)\n",
    "    data.columns = [f\"{ticker}_{col}\" for col in data.columns]  # Prefix columns with ticker\n",
    "    return data\n",
    "\n",
    "# Re-fetch data with updated column naming\n",
    "bitcoin = fetch_stock_data('BTC-USD')\n",
    "\n",
    "# Debug: Check column names in bitcoin DataFrame\n",
    "print(\"Flattened Columns in Bitcoin DataFrame:\")\n",
    "print(bitcoin.columns)\n",
    "\n",
    "# Adjust for closing price column\n",
    "closing_column = None\n",
    "for col in bitcoin.columns:\n",
    "    if 'close' in col.lower():  # Match 'Close' case-insensitively\n",
    "        closing_column = col\n",
    "        break\n",
    "\n",
    "if closing_column is None:\n",
    "    raise KeyError(\"Column for closing prices not found. Available columns: \" + ', '.join(bitcoin.columns))\n",
    "\n",
    "# Example plot: Bitcoin prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x=bitcoin.index, y=bitcoin[closing_column], color=\"orange\")\n",
    "plt.title(\"Bitcoin Prices in USD\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example correlation heatmap for FAANG stocks\n",
    "faang.columns = ['META', 'AAPL', 'AMZN', 'NFLX', 'GOOG']  # Ensure correct column names\n",
    "correlation_matrix = faang.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap for FAANG Stocks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example pairplot for FAANG stocks\n",
    "faang_viz = faang.reset_index(drop=True)  # Reset index for visualization\n",
    "sns.pairplot(faang_viz, diag_kind=\"kde\", kind=\"scatter\", corner=True)\n",
    "plt.suptitle(\"FAANG Stocks Pairplot\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing cumulative returns\n",
    "from cycler import cycler\n",
    "\n",
    "# Black-and-white visualization cycle\n",
    "bw_viz_cycler = (\n",
    "    cycler(color=[plt.get_cmap('tab10')(x / 10) for x in range(10)])\n",
    "    + cycler(linestyle=['dashed', 'solid', 'dashdot', 'dotted', 'solid'] * 2)\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axes[0].set_prop_cycle(bw_viz_cycler)\n",
    "\n",
    "# Function to calculate cumulative returns\n",
    "def calculate_cumulative_returns(data):\n",
    "    returns = data.pct_change().fillna(0)  # Calculate daily returns\n",
    "    cumulative_returns = (1 + returns).cumprod() - 1  # Calculate cumulative returns\n",
    "    return cumulative_returns\n",
    "\n",
    "# Calculate cumulative returns for all assets\n",
    "cumulative_returns = calculate_cumulative_returns(all_assets)\n",
    "\n",
    "for name in cumulative_returns.columns:\n",
    "    if name == 'Bitcoin':\n",
    "        cumulative_returns[name].plot(ax=axes[1], label=name, legend=True)\n",
    "    else:\n",
    "        cumulative_returns[name].plot(ax=axes[0], label=name, legend=True)\n",
    "\n",
    "fig.suptitle('Cumulative Returns')\n",
    "axes[0].set_title('Non-Bitcoin Assets')\n",
    "axes[1].set_title('Bitcoin')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "class StockModeler:\n",
    "\t@staticmethod\n",
    "\tdef decompose(data, period):\n",
    "\t\treturn sm.tsa.seasonal_decompose(data['Close'], period=period)\n",
    "\n",
    "# Perform decomposition\n",
    "decomposition = StockModeler.decompose(nflx, 20) # 20 period frequency\n",
    "fig = decomposition.plot()\n",
    "fig.suptitle('Netflix Stock Price Time Series Decomposition', y=1)\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(10)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 'Close' column for the autocorrelation plot\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "if 'Close' in nflx.columns:\n",
    "    autocorrelation_plot(nflx['Close'])\n",
    "    plt.title(\"Autocorrelation Plot for Netflix Closing Prices\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"The 'Close' column is not available in the Netflix DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag plot for Netflix stock\n",
    "data_to_plot = nflx['Close'] if 'Close' in nflx.columns else None\n",
    "if data_to_plot is not None:\n",
    "    lag_plot(data_to_plot)\n",
    "    plt.title(\"Lag Plot for Netflix Closing Prices\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"The 'Close' column is not available for Netflix data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Prepare the Netflix data for ARIMA modeling\n",
    "if 'Close' in nflx.columns:\n",
    "    nflx_close = nflx['Close'].dropna()  # Ensure no missing values\n",
    "else:\n",
    "    raise KeyError(\"The 'Close' column is not available in the Netflix data.\")\n",
    "\n",
    "# Define ARIMA parameters\n",
    "ar_order = 10  # Autoregressive term\n",
    "i_order = 1    # Differencing term\n",
    "ma_order = 5   # Moving average term\n",
    "\n",
    "# Fit the ARIMA model\n",
    "arima_model = ARIMA(nflx_close, order=(ar_order, i_order, ma_order))\n",
    "arima_result = arima_model.fit()\n",
    "\n",
    "# Display the ARIMA model summary\n",
    "print(arima_result.summary())\n",
    "\n",
    "# Plot the fitted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(nflx_close, label=\"Original Data\")\n",
    "plt.plot(arima_result.fittedvalues, label=\"Fitted Values\", color=\"red\")\n",
    "plt.title(\"ARIMA Model Fitted Values\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA model is not available.\n"
     ]
    }
   ],
   "source": [
    "# Plot ARIMA residuals\n",
    "if 'arima_model' in locals():\n",
    "    residuals = arima_model.resid\n",
    "\n",
    "    # Residual time series plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(residuals, color='blue')\n",
    "    plt.axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "    plt.title(\"ARIMA Model Residuals\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.show()\n",
    "\n",
    "    # Residuals distribution plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(residuals, kde=True, bins=30, color='blue')\n",
    "    plt.title(\"Residuals Distribution\")\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ARIMA model is not available.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
